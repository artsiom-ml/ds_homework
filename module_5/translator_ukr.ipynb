{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MIEGXF8oM9tt",
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "3c8f71ee-eead-4774-c95b-67d139fa9155"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from io import open\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kyNnJyruM9t1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class LanguageVocabulary(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading lines...\n",
      "Read 156173 sentence pairs\n",
      "Trimmed to 155413 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 10014\n",
      "ukr 30331\n",
      "['tom is hiding behind the couch .', 'том ховається за диваном .']\n"
     ]
    }
   ],
   "source": [
    "class Preprocessing:\n",
    "\n",
    "    DATA_DIR = '/content/drive/My Drive/Colab Notebooks/translation_data/'\n",
    "\n",
    "    def __init__(self, lang, reverse=False, max_length_seq = 15):\n",
    "        self.lang = lang\n",
    "        self.source_path = os.path.join(self.DATA_DIR, lang + '.txt')\n",
    "        self.target_path = os.path.join(self.DATA_DIR, 'eng_' + lang + '.txt')\n",
    "        self.max_length = max_length_seq\n",
    "        self.reverse = reverse\n",
    "\n",
    "    @staticmethod\n",
    "    def unicode_to_ascii(s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "\n",
    "    def normalize_string(self, s):\n",
    "        s = self.unicode_to_ascii(s.lower().strip())\n",
    "        s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "        s = re.sub(r\"[^a-zA-Zа-яА-Яє-їЄ-Ї.!?]+\", r\" \", s)\n",
    "        # s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "        return s\n",
    "\n",
    "    def prepare_file(self, source_path, target_path):\n",
    "        with open(source_path, encoding='utf-8') as file1, open(target_path, 'w', encoding='utf-8') as file2:\n",
    "            for line in file1:\n",
    "                file2.write(line.strip().split('\\t')[0] + '\\t' + line.strip().split('\\t')[1] + '\\n')\n",
    "\n",
    "    def prepare_data(self, lang1, lang2, reverse=False):\n",
    "        input_lang, output_lang, pairs = self.read_languages(lang1, lang2, reverse)\n",
    "        print(\"Read %s sentence pairs\" % len(pairs))\n",
    "        pairs = self.filter_pairs(pairs)\n",
    "        print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "        print(\"Counting words...\")\n",
    "        for pair in pairs:\n",
    "            input_lang.add_sentence(pair[0])\n",
    "            output_lang.add_sentence(pair[1])\n",
    "        print(\"Counted words:\")\n",
    "        print(input_lang.name, input_lang.n_words)\n",
    "        print(output_lang.name, output_lang.n_words)\n",
    "        return input_lang, output_lang, pairs\n",
    "\n",
    "    def read_languages(self, lang1, lang2, reverse=False):\n",
    "        print(\"Reading lines...\")\n",
    "        path = os.path.join(self.DATA_DIR, '%s-%s.txt' % (lang1, lang2))\n",
    "        lines = open(path, encoding='utf-8').read().strip().split('\\n')\n",
    "        pairs = [[self.normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "        if reverse:\n",
    "            pairs = [list(reversed(p)) for p in pairs]\n",
    "            input_lang = LanguageVocabulary(lang2)\n",
    "            output_lang = LanguageVocabulary(lang1)\n",
    "        else:\n",
    "            input_lang = LanguageVocabulary(lang1)\n",
    "            output_lang = LanguageVocabulary(lang2)\n",
    "        return input_lang, output_lang, pairs\n",
    "\n",
    "    def get_data(self):\n",
    "        self.prepare_file(self.source_path, self.target_path)\n",
    "        input_lang, output_lang, pairs = self.prepare_data('eng', self.lang, self.reverse)\n",
    "        return input_lang, output_lang, pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_pair(p, max_length):\n",
    "        return len(p[0].split(' ')) < max_length and len(p[1].split(' ')) < max_length\n",
    "\n",
    "    def filter_pairs(self, pairs):\n",
    "        return [pair for pair in pairs if self.filter_pair(pair, self.max_length)]\n",
    "\n",
    "MAX_LENGTH = 15\n",
    "input_lang, output_lang, pairs = Preprocessing(lang='ukr', max_length_seq=MAX_LENGTH).get_data()\n",
    "print(random.choice(pairs))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-28uSJe3bYFB",
    "outputId": "bd343930-e196-449e-c30a-e01ae6b772d0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "m9vm9QBWM9uI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "z6gGPtXFM9uQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8Fn8VDv8M9uS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EKsdwPmSM9uU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / percent\n",
    "    rs = es - s\n",
    "    return '%s (- eta: %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "C_z_k5IiM9uX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[epoch - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_iters),\n",
    "                                         epoch, epoch / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0JXG-RzCM9uZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3Bxf45h6M9ud",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for i in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
    "            encoder_outputs[i] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1qUmQIGwM9uf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_56t10oM9uh",
    "outputId": "9f43571d-0cbc-4223-a72c-b3956503ef85",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1m 26s (- eta: 41m 39s) (5000 3%) 5.0398\n",
      "2m 44s (- eta: 38m 23s) (10000 6%) 4.5353\n",
      "4m 3s (- eta: 36m 35s) (15000 10%) 4.2502\n",
      "5m 23s (- eta: 35m 1s) (20000 13%) 4.0189\n",
      "6m 42s (- eta: 33m 34s) (25000 16%) 3.8582\n",
      "8m 2s (- eta: 32m 8s) (30000 20%) 3.7093\n",
      "9m 21s (- eta: 30m 44s) (35000 23%) 3.6226\n",
      "10m 40s (- eta: 29m 22s) (40000 26%) 3.5143\n",
      "12m 0s (- eta: 28m 0s) (45000 30%) 3.4167\n",
      "13m 19s (- eta: 26m 39s) (50000 33%) 3.3383\n",
      "14m 39s (- eta: 25m 18s) (55000 36%) 3.2709\n",
      "15m 59s (- eta: 23m 58s) (60000 40%) 3.1636\n",
      "17m 18s (- eta: 22m 38s) (65000 43%) 3.1525\n",
      "18m 38s (- eta: 21m 18s) (70000 46%) 3.1129\n",
      "19m 58s (- eta: 19m 58s) (75000 50%) 3.0359\n",
      "21m 17s (- eta: 18m 38s) (80000 53%) 2.9818\n",
      "22m 37s (- eta: 17m 18s) (85000 56%) 2.9540\n",
      "23m 57s (- eta: 15m 58s) (90000 60%) 2.8836\n",
      "25m 16s (- eta: 14m 38s) (95000 63%) 2.9086\n",
      "26m 36s (- eta: 13m 18s) (100000 66%) 2.7977\n",
      "27m 56s (- eta: 11m 58s) (105000 70%) 2.7822\n",
      "29m 16s (- eta: 10m 38s) (110000 73%) 2.7341\n",
      "30m 36s (- eta: 9m 18s) (115000 76%) 2.7067\n",
      "31m 55s (- eta: 7m 58s) (120000 80%) 2.6420\n",
      "33m 16s (- eta: 6m 39s) (125000 83%) 2.6280\n",
      "34m 36s (- eta: 5m 19s) (130000 86%) 2.6235\n",
      "35m 56s (- eta: 3m 59s) (135000 90%) 2.5915\n",
      "37m 16s (- eta: 2m 39s) (140000 93%) 2.5271\n",
      "38m 37s (- eta: 1m 19s) (145000 96%) 2.5807\n",
      "39m 57s (- eta: 0m 0s) (150000 100%) 2.5009\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "trainIters(encoder1, decoder1, 150000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xEoEylSyM9uj",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "outputId": "50edbfb2-bdc0-4076-bb53-e633c6bb5504"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "> what happened last month ?\n",
      "= що сталося минулого місяця ?\n",
      "< що трапилося сталося ? <EOS>\n",
      "\n",
      "> it didn t work .\n",
      "= це не спрацювало .\n",
      "< це не працює . <EOS>\n",
      "\n",
      "> did tom say where he wanted to go ?\n",
      "= том сказав куди він хоче піти ?\n",
      "< том сказав куди куди пішов ? <EOS>\n",
      "\n",
      "> tom will buy a new bicycle .\n",
      "= том купить новии велосипед .\n",
      "< том купив новии новии . <EOS>\n",
      "\n",
      "> what s up dude ?\n",
      "= як справи друже ?\n",
      "< що у мене ? ? <EOS>\n",
      "\n",
      "> sell the diamonds .\n",
      "= продаи діаманти .\n",
      "< кішка пістолет . <EOS>\n",
      "\n",
      "> i like ponies .\n",
      "= я люблю поні .\n",
      "< я мені . . <EOS>\n",
      "\n",
      "> please call the police .\n",
      "= викличте поліцію будь ласка .\n",
      "< будь ласка ласка . <EOS>\n",
      "\n",
      "> i don t think that there will be any problems .\n",
      "= не думаю що будуть які небудь проблеми .\n",
      "< не думаю що що є що я маю є <EOS>\n",
      "\n",
      "> i don t believe such things exist .\n",
      "= я не вірю що такі речі існують .\n",
      "< я не думаю що що у . . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_enc(path):\n",
    "    model = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_dec(path):\n",
    "    model = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "save_model(encoder1, 'encoder_ukr_eng_v3.pt')\n",
    "save_model(decoder1, 'decoder_ukr_eng_v3.pt')"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "onH6aaCmbYFM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# encoder2 = load_enc('encoder_bel_eng.pt')\n",
    "# decoder2 = load_dec('decoder_bel_eng.pt')\n",
    "#\n",
    "# evaluateRandomly(encoder2, decoder2)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "6mYNwDkJbYFM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "cehVOKqNbYFN"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "23ceb7112fbf9d0e38ecbf60d6e6d5e2dcebcc82200eeb1e5a5d5f9ffb9e27ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}