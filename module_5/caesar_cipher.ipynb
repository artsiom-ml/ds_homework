{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import torch\n",
    "from nltk import tokenize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "FILE_NAME = './data/nietzsche.txt'\n",
    "CAESAR_OFFSET = 2\n",
    "CHARS = list('abcdefghijklmnopqrstuvwxyz ')\n",
    "INDEX_TO_CHAR = [w for w in sorted(CHARS)]\n",
    "CHAR_TO_INDEX = {w: i for i, w in enumerate(INDEX_TO_CHAR)}\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 400"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def load_and_vanilla_preprocess(txt_path):\n",
    "    with open(txt_path, encoding='utf-8') as txt_file:\n",
    "        text = txt_file.read().lower()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    sentences = [s for s in sentences if len(s) < 1000]\n",
    "    sentences = [re.sub('[^a-z ]', ' ', s) for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "def caesor(s, shift):\n",
    "    res = ''\n",
    "    for c in s:\n",
    "        if c not in CHARS:\n",
    "            res += ' '\n",
    "        else:\n",
    "            res += CHARS[(CHARS.index(c.lower()) + shift) % len(CHARS)]\n",
    "    return res\n",
    "\n",
    "text = load_and_vanilla_preprocess(FILE_NAME)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def vectorize(text):\n",
    "    max_length = len(max(text, key=len))\n",
    "    X = torch.zeros((len(text), max_length), dtype=int)\n",
    "    Y = torch.zeros((len(text), max_length), dtype=int)\n",
    "\n",
    "    for i in range(len(text)):\n",
    "        for j, w in enumerate(text[i]):\n",
    "            X[i, j] = CHAR_TO_INDEX.get(caesor(w, CAESAR_OFFSET), CHAR_TO_INDEX[' '])\n",
    "            Y[i, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX[' '])\n",
    "    return X, Y\n",
    "\n",
    "X, Y = vectorize(text)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X, Y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n",
    "test_dl = torch.utils.data.DataLoader(test_dataset, BATCH_SIZE, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class RNNModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(len(CHARS), 32)\n",
    "        self.rnn = torch.nn.RNN(32, 128, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(128, len(CHARS))\n",
    "\n",
    "    def forward(self, sentence, state=None):\n",
    "        embed = self.embed(sentence)\n",
    "        o, h = self.rnn(embed)\n",
    "        return self.linear(o)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "model = RNNModel()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.05)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 9.8585, acc: 0.6646 | test loss: 1.4391, test acc: 0.8496 | 6.78 sec.\n",
      "Epoch: 1, loss: 3.4123, acc: 0.8379 | test loss: 1.3063, test acc: 0.8302 | 6.77 sec.\n",
      "Epoch: 2, loss: 3.0658, acc: 0.8384 | test loss: 1.1772, test acc: 0.8426 | 6.79 sec.\n",
      "Epoch: 3, loss: 2.9296, acc: 0.8421 | test loss: 1.1244, test acc: 0.8475 | 7.44 sec.\n",
      "Epoch: 4, loss: 2.8286, acc: 0.8460 | test loss: 1.0720, test acc: 0.8539 | 7.27 sec.\n",
      "Epoch: 5, loss: 2.7241, acc: 0.8513 | test loss: 1.0846, test acc: 0.8515 | 7.14 sec.\n",
      "Epoch: 6, loss: 2.6863, acc: 0.8533 | test loss: 1.0957, test acc: 0.8505 | 6.84 sec.\n",
      "Epoch: 7, loss: 2.6019, acc: 0.8586 | test loss: 0.9401, test acc: 0.8725 | 7.10 sec.\n",
      "Epoch: 8, loss: 2.5275, acc: 0.8634 | test loss: 1.0785, test acc: 0.8543 | 6.59 sec.\n",
      "Epoch: 9, loss: 2.4814, acc: 0.8674 | test loss: 0.9372, test acc: 0.8754 | 8.01 sec.\n",
      "Epoch: 10, loss: 2.4097, acc: 0.8747 | test loss: 0.9400, test acc: 0.8800 | 7.24 sec.\n",
      "Epoch: 11, loss: 2.3191, acc: 0.8845 | test loss: 0.8570, test acc: 0.8959 | 6.83 sec.\n",
      "Epoch: 12, loss: 2.2865, acc: 0.8930 | test loss: 0.8696, test acc: 0.9026 | 8.25 sec.\n",
      "Epoch: 13, loss: 2.2371, acc: 0.9023 | test loss: 0.9392, test acc: 0.9007 | 7.33 sec.\n",
      "Epoch: 14, loss: 2.1804, acc: 0.9112 | test loss: 0.8317, test acc: 0.9180 | 6.97 sec.\n",
      "Epoch: 15, loss: 2.0978, acc: 0.9208 | test loss: 0.8198, test acc: 0.9249 | 7.82 sec.\n",
      "Epoch: 16, loss: 2.0652, acc: 0.9271 | test loss: 0.8074, test acc: 0.9313 | 7.31 sec.\n",
      "Epoch: 17, loss: 2.0223, acc: 0.9328 | test loss: 0.7423, test acc: 0.9396 | 7.77 sec.\n",
      "Epoch: 18, loss: 1.9699, acc: 0.9374 | test loss: 0.7935, test acc: 0.9376 | 7.24 sec.\n",
      "Epoch: 19, loss: 1.9163, acc: 0.9411 | test loss: 0.7076, test acc: 0.9464 | 7.47 sec.\n",
      "Epoch: 20, loss: 1.8856, acc: 0.9439 | test loss: 0.7502, test acc: 0.9449 | 7.27 sec.\n",
      "Epoch: 21, loss: 1.8313, acc: 0.9472 | test loss: 0.6750, test acc: 0.9512 | 7.26 sec.\n",
      "Epoch: 22, loss: 1.7870, acc: 0.9496 | test loss: 0.6609, test acc: 0.9534 | 6.98 sec.\n",
      "Epoch: 23, loss: 1.7635, acc: 0.9512 | test loss: 0.6815, test acc: 0.9525 | 7.01 sec.\n",
      "Epoch: 24, loss: 1.7051, acc: 0.9533 | test loss: 0.6151, test acc: 0.9575 | 7.00 sec.\n",
      "Epoch: 25, loss: 1.6808, acc: 0.9543 | test loss: 0.6422, test acc: 0.9565 | 7.14 sec.\n",
      "Epoch: 26, loss: 1.6347, acc: 0.9560 | test loss: 0.6575, test acc: 0.9556 | 7.42 sec.\n",
      "Epoch: 27, loss: 1.6111, acc: 0.9572 | test loss: 0.6205, test acc: 0.9593 | 7.28 sec.\n",
      "Epoch: 28, loss: 1.5546, acc: 0.9592 | test loss: 0.5717, test acc: 0.9622 | 7.85 sec.\n",
      "Epoch: 29, loss: 1.5219, acc: 0.9601 | test loss: 0.5829, test acc: 0.9614 | 8.57 sec.\n",
      "Epoch: 30, loss: 1.4871, acc: 0.9612 | test loss: 0.6226, test acc: 0.9595 | 8.39 sec.\n",
      "Epoch: 31, loss: 1.4601, acc: 0.9621 | test loss: 0.5106, test acc: 0.9664 | 7.67 sec.\n",
      "Epoch: 32, loss: 1.4244, acc: 0.9632 | test loss: 0.5305, test acc: 0.9655 | 6.96 sec.\n",
      "Epoch: 33, loss: 1.3994, acc: 0.9639 | test loss: 0.4991, test acc: 0.9676 | 7.38 sec.\n",
      "Epoch: 34, loss: 1.3806, acc: 0.9645 | test loss: 0.5014, test acc: 0.9675 | 6.94 sec.\n",
      "Epoch: 35, loss: 1.3440, acc: 0.9656 | test loss: 0.5333, test acc: 0.9657 | 6.92 sec.\n",
      "Epoch: 36, loss: 1.3242, acc: 0.9661 | test loss: 0.5233, test acc: 0.9661 | 7.17 sec.\n",
      "Epoch: 37, loss: 1.2835, acc: 0.9670 | test loss: 0.5038, test acc: 0.9670 | 7.30 sec.\n",
      "Epoch: 38, loss: 1.2624, acc: 0.9674 | test loss: 0.5099, test acc: 0.9666 | 7.89 sec.\n",
      "Epoch: 39, loss: 1.2458, acc: 0.9676 | test loss: 0.5021, test acc: 0.9670 | 7.02 sec.\n",
      "Epoch: 40, loss: 1.2165, acc: 0.9683 | test loss: 0.4858, test acc: 0.9685 | 7.27 sec.\n",
      "Epoch: 41, loss: 1.1934, acc: 0.9690 | test loss: 0.4891, test acc: 0.9675 | 7.06 sec.\n",
      "Epoch: 42, loss: 1.1786, acc: 0.9692 | test loss: 0.4539, test acc: 0.9702 | 6.92 sec.\n",
      "Epoch: 43, loss: 1.1409, acc: 0.9704 | test loss: 0.4311, test acc: 0.9718 | 6.94 sec.\n",
      "Epoch: 44, loss: 1.1104, acc: 0.9714 | test loss: 0.4153, test acc: 0.9733 | 7.19 sec.\n",
      "Epoch: 45, loss: 1.1036, acc: 0.9718 | test loss: 0.4289, test acc: 0.9725 | 7.31 sec.\n",
      "Epoch: 46, loss: 1.0796, acc: 0.9727 | test loss: 0.4037, test acc: 0.9742 | 7.21 sec.\n",
      "Epoch: 47, loss: 1.0522, acc: 0.9736 | test loss: 0.3754, test acc: 0.9765 | 6.99 sec.\n",
      "Epoch: 48, loss: 1.0371, acc: 0.9745 | test loss: 0.4527, test acc: 0.9720 | 6.92 sec.\n",
      "Epoch: 49, loss: 1.0160, acc: 0.9756 | test loss: 0.3743, test acc: 0.9774 | 8.00 sec.\n",
      "Epoch: 50, loss: 1.0039, acc: 0.9764 | test loss: 0.3955, test acc: 0.9764 | 7.28 sec.\n",
      "Epoch: 51, loss: 0.9781, acc: 0.9771 | test loss: 0.3869, test acc: 0.9772 | 8.33 sec.\n",
      "Epoch: 52, loss: 0.9718, acc: 0.9772 | test loss: 0.3698, test acc: 0.9779 | 8.81 sec.\n",
      "Epoch: 53, loss: 0.9508, acc: 0.9777 | test loss: 0.3576, test acc: 0.9788 | 6.70 sec.\n",
      "Epoch: 54, loss: 0.9461, acc: 0.9778 | test loss: 0.3331, test acc: 0.9806 | 7.00 sec.\n",
      "Epoch: 55, loss: 0.9151, acc: 0.9786 | test loss: 0.3486, test acc: 0.9800 | 10.40 sec.\n",
      "Epoch: 56, loss: 0.9208, acc: 0.9786 | test loss: 0.3605, test acc: 0.9788 | 8.26 sec.\n",
      "Epoch: 57, loss: 0.8857, acc: 0.9795 | test loss: 0.3227, test acc: 0.9813 | 6.85 sec.\n",
      "Epoch: 58, loss: 0.8667, acc: 0.9801 | test loss: 0.3308, test acc: 0.9806 | 6.57 sec.\n",
      "Epoch: 59, loss: 0.8607, acc: 0.9802 | test loss: 0.3588, test acc: 0.9794 | 6.52 sec.\n",
      "Epoch: 60, loss: 0.8462, acc: 0.9807 | test loss: 0.3468, test acc: 0.9800 | 6.40 sec.\n",
      "Epoch: 61, loss: 0.8373, acc: 0.9809 | test loss: 0.3027, test acc: 0.9826 | 7.21 sec.\n",
      "Epoch: 62, loss: 0.8267, acc: 0.9812 | test loss: 0.3040, test acc: 0.9829 | 7.49 sec.\n",
      "Epoch: 63, loss: 0.8091, acc: 0.9818 | test loss: 0.3391, test acc: 0.9812 | 6.95 sec.\n",
      "Epoch: 64, loss: 0.7911, acc: 0.9822 | test loss: 0.3100, test acc: 0.9825 | 7.27 sec.\n",
      "Epoch: 65, loss: 0.7874, acc: 0.9823 | test loss: 0.3095, test acc: 0.9827 | 7.42 sec.\n",
      "Epoch: 66, loss: 0.7833, acc: 0.9824 | test loss: 0.3082, test acc: 0.9826 | 7.70 sec.\n",
      "Epoch: 67, loss: 0.7637, acc: 0.9828 | test loss: 0.2871, test acc: 0.9835 | 7.09 sec.\n",
      "Epoch: 68, loss: 0.7539, acc: 0.9829 | test loss: 0.2983, test acc: 0.9827 | 7.80 sec.\n",
      "Epoch: 69, loss: 0.7465, acc: 0.9830 | test loss: 0.2859, test acc: 0.9837 | 6.67 sec.\n",
      "Epoch: 70, loss: 0.7329, acc: 0.9832 | test loss: 0.2700, test acc: 0.9844 | 7.83 sec.\n",
      "Epoch: 71, loss: 0.7226, acc: 0.9834 | test loss: 0.2762, test acc: 0.9841 | 6.72 sec.\n",
      "Epoch: 72, loss: 0.7117, acc: 0.9837 | test loss: 0.2731, test acc: 0.9845 | 6.75 sec.\n",
      "Epoch: 73, loss: 0.6960, acc: 0.9841 | test loss: 0.2718, test acc: 0.9845 | 7.97 sec.\n",
      "Epoch: 74, loss: 0.7017, acc: 0.9840 | test loss: 0.2861, test acc: 0.9835 | 6.56 sec.\n",
      "Epoch: 75, loss: 0.6857, acc: 0.9844 | test loss: 0.2404, test acc: 0.9862 | 6.68 sec.\n",
      "Epoch: 76, loss: 0.6846, acc: 0.9843 | test loss: 0.2631, test acc: 0.9849 | 8.37 sec.\n",
      "Epoch: 77, loss: 0.6706, acc: 0.9846 | test loss: 0.2660, test acc: 0.9847 | 7.22 sec.\n",
      "Epoch: 78, loss: 0.6596, acc: 0.9849 | test loss: 0.2507, test acc: 0.9857 | 8.38 sec.\n",
      "Epoch: 79, loss: 0.6517, acc: 0.9850 | test loss: 0.2623, test acc: 0.9847 | 6.85 sec.\n",
      "Epoch: 80, loss: 0.6409, acc: 0.9851 | test loss: 0.2437, test acc: 0.9859 | 7.24 sec.\n",
      "Epoch: 81, loss: 0.6316, acc: 0.9852 | test loss: 0.2346, test acc: 0.9862 | 7.66 sec.\n",
      "Epoch: 82, loss: 0.6275, acc: 0.9852 | test loss: 0.2436, test acc: 0.9855 | 7.53 sec.\n",
      "Epoch: 83, loss: 0.6249, acc: 0.9852 | test loss: 0.2488, test acc: 0.9849 | 7.40 sec.\n",
      "Epoch: 84, loss: 0.6102, acc: 0.9854 | test loss: 0.2172, test acc: 0.9870 | 7.51 sec.\n",
      "Epoch: 85, loss: 0.5995, acc: 0.9856 | test loss: 0.2209, test acc: 0.9865 | 7.05 sec.\n",
      "Epoch: 86, loss: 0.6017, acc: 0.9855 | test loss: 0.2217, test acc: 0.9866 | 7.10 sec.\n",
      "Epoch: 87, loss: 0.5897, acc: 0.9856 | test loss: 0.2257, test acc: 0.9860 | 7.55 sec.\n",
      "Epoch: 88, loss: 0.5827, acc: 0.9857 | test loss: 0.2143, test acc: 0.9867 | 8.03 sec.\n",
      "Epoch: 89, loss: 0.5757, acc: 0.9857 | test loss: 0.2267, test acc: 0.9859 | 6.58 sec.\n",
      "Epoch: 90, loss: 0.5748, acc: 0.9856 | test loss: 0.2217, test acc: 0.9860 | 7.02 sec.\n",
      "Epoch: 91, loss: 0.5640, acc: 0.9858 | test loss: 0.2190, test acc: 0.9862 | 6.79 sec.\n",
      "Epoch: 92, loss: 0.5538, acc: 0.9861 | test loss: 0.2264, test acc: 0.9857 | 7.27 sec.\n",
      "Epoch: 93, loss: 0.5537, acc: 0.9860 | test loss: 0.2039, test acc: 0.9870 | 6.89 sec.\n",
      "Epoch: 94, loss: 0.5442, acc: 0.9861 | test loss: 0.2166, test acc: 0.9862 | 7.30 sec.\n",
      "Epoch: 95, loss: 0.5406, acc: 0.9862 | test loss: 0.1962, test acc: 0.9872 | 7.10 sec.\n",
      "Epoch: 96, loss: 0.5289, acc: 0.9864 | test loss: 0.2024, test acc: 0.9869 | 9.39 sec.\n",
      "Epoch: 97, loss: 0.5252, acc: 0.9867 | test loss: 0.1980, test acc: 0.9875 | 7.57 sec.\n",
      "Epoch: 98, loss: 0.5202, acc: 0.9868 | test loss: 0.1889, test acc: 0.9881 | 7.16 sec.\n",
      "Epoch: 99, loss: 0.5127, acc: 0.9869 | test loss: 0.1990, test acc: 0.9872 | 6.59 sec.\n",
      "Epoch: 100, loss: 0.5114, acc: 0.9870 | test loss: 0.2074, test acc: 0.9868 | 6.58 sec.\n",
      "Epoch: 101, loss: 0.5007, acc: 0.9873 | test loss: 0.1948, test acc: 0.9877 | 6.95 sec.\n",
      "Epoch: 102, loss: 0.4981, acc: 0.9874 | test loss: 0.2001, test acc: 0.9868 | 7.25 sec.\n",
      "Epoch: 103, loss: 0.4974, acc: 0.9874 | test loss: 0.1982, test acc: 0.9873 | 9.76 sec.\n",
      "Epoch: 104, loss: 0.4865, acc: 0.9877 | test loss: 0.1835, test acc: 0.9880 | 19.40 sec.\n",
      "Epoch: 105, loss: 0.4820, acc: 0.9878 | test loss: 0.1745, test acc: 0.9888 | 12.90 sec.\n",
      "Epoch: 106, loss: 0.4786, acc: 0.9879 | test loss: 0.1864, test acc: 0.9881 | 7.85 sec.\n",
      "Epoch: 107, loss: 0.4662, acc: 0.9881 | test loss: 0.1823, test acc: 0.9882 | 9.11 sec.\n",
      "Epoch: 108, loss: 0.4704, acc: 0.9880 | test loss: 0.1897, test acc: 0.9877 | 7.84 sec.\n",
      "Epoch: 109, loss: 0.4583, acc: 0.9883 | test loss: 0.1773, test acc: 0.9886 | 7.61 sec.\n",
      "Epoch: 110, loss: 0.4593, acc: 0.9882 | test loss: 0.2004, test acc: 0.9867 | 8.56 sec.\n",
      "Epoch: 111, loss: 0.4542, acc: 0.9883 | test loss: 0.1673, test acc: 0.9892 | 8.44 sec.\n",
      "Epoch: 112, loss: 0.4447, acc: 0.9885 | test loss: 0.1817, test acc: 0.9882 | 7.97 sec.\n",
      "Epoch: 113, loss: 0.4437, acc: 0.9885 | test loss: 0.1765, test acc: 0.9886 | 9.06 sec.\n",
      "Epoch: 114, loss: 0.4349, acc: 0.9888 | test loss: 0.1767, test acc: 0.9884 | 7.17 sec.\n",
      "Epoch: 115, loss: 0.4380, acc: 0.9886 | test loss: 0.1747, test acc: 0.9882 | 7.04 sec.\n",
      "Epoch: 116, loss: 0.4325, acc: 0.9887 | test loss: 0.1684, test acc: 0.9889 | 7.55 sec.\n",
      "Epoch: 117, loss: 0.4260, acc: 0.9889 | test loss: 0.1723, test acc: 0.9887 | 7.79 sec.\n",
      "Epoch: 118, loss: 0.4239, acc: 0.9888 | test loss: 0.1727, test acc: 0.9882 | 7.66 sec.\n",
      "Epoch: 119, loss: 0.4156, acc: 0.9891 | test loss: 0.1639, test acc: 0.9889 | 7.17 sec.\n",
      "Epoch: 120, loss: 0.4151, acc: 0.9891 | test loss: 0.1735, test acc: 0.9884 | 8.09 sec.\n",
      "Epoch: 121, loss: 0.4085, acc: 0.9892 | test loss: 0.1798, test acc: 0.9878 | 7.08 sec.\n",
      "Epoch: 122, loss: 0.4065, acc: 0.9892 | test loss: 0.1513, test acc: 0.9897 | 7.85 sec.\n",
      "Epoch: 123, loss: 0.3987, acc: 0.9894 | test loss: 0.1687, test acc: 0.9885 | 8.59 sec.\n",
      "Epoch: 124, loss: 0.3971, acc: 0.9893 | test loss: 0.1555, test acc: 0.9891 | 8.18 sec.\n",
      "Epoch: 125, loss: 0.3924, acc: 0.9895 | test loss: 0.1559, test acc: 0.9892 | 8.07 sec.\n",
      "Epoch: 126, loss: 0.3906, acc: 0.9894 | test loss: 0.1567, test acc: 0.9890 | 7.36 sec.\n",
      "Epoch: 127, loss: 0.3896, acc: 0.9895 | test loss: 0.1578, test acc: 0.9887 | 7.06 sec.\n",
      "Epoch: 128, loss: 0.3832, acc: 0.9895 | test loss: 0.1532, test acc: 0.9896 | 7.24 sec.\n",
      "Epoch: 129, loss: 0.3835, acc: 0.9895 | test loss: 0.1604, test acc: 0.9889 | 8.01 sec.\n",
      "Epoch: 130, loss: 0.3813, acc: 0.9895 | test loss: 0.1351, test acc: 0.9904 | 6.62 sec.\n",
      "Epoch: 131, loss: 0.3721, acc: 0.9898 | test loss: 0.1426, test acc: 0.9899 | 6.48 sec.\n",
      "Epoch: 132, loss: 0.3712, acc: 0.9897 | test loss: 0.1470, test acc: 0.9895 | 6.55 sec.\n",
      "Epoch: 133, loss: 0.3655, acc: 0.9899 | test loss: 0.1582, test acc: 0.9885 | 6.49 sec.\n",
      "Epoch: 134, loss: 0.3652, acc: 0.9898 | test loss: 0.1392, test acc: 0.9900 | 6.73 sec.\n",
      "Epoch: 135, loss: 0.3563, acc: 0.9900 | test loss: 0.1473, test acc: 0.9895 | 7.26 sec.\n",
      "Epoch: 136, loss: 0.3592, acc: 0.9899 | test loss: 0.1417, test acc: 0.9896 | 6.78 sec.\n",
      "Epoch: 137, loss: 0.3509, acc: 0.9901 | test loss: 0.1331, test acc: 0.9903 | 6.55 sec.\n",
      "Epoch: 138, loss: 0.3520, acc: 0.9900 | test loss: 0.1410, test acc: 0.9901 | 6.61 sec.\n",
      "Epoch: 139, loss: 0.3495, acc: 0.9900 | test loss: 0.1262, test acc: 0.9907 | 6.78 sec.\n",
      "Epoch: 140, loss: 0.3452, acc: 0.9901 | test loss: 0.1403, test acc: 0.9897 | 6.64 sec.\n",
      "Epoch: 141, loss: 0.3421, acc: 0.9902 | test loss: 0.1332, test acc: 0.9901 | 6.59 sec.\n",
      "Epoch: 142, loss: 0.3343, acc: 0.9904 | test loss: 0.1408, test acc: 0.9893 | 6.42 sec.\n",
      "Epoch: 143, loss: 0.3411, acc: 0.9901 | test loss: 0.1362, test acc: 0.9898 | 6.47 sec.\n",
      "Epoch: 144, loss: 0.3315, acc: 0.9904 | test loss: 0.1286, test acc: 0.9902 | 6.63 sec.\n",
      "Epoch: 145, loss: 0.3296, acc: 0.9905 | test loss: 0.1281, test acc: 0.9903 | 6.88 sec.\n",
      "Epoch: 146, loss: 0.3288, acc: 0.9905 | test loss: 0.1179, test acc: 0.9913 | 6.66 sec.\n",
      "Epoch: 147, loss: 0.3257, acc: 0.9906 | test loss: 0.1274, test acc: 0.9903 | 6.61 sec.\n",
      "Epoch: 148, loss: 0.3232, acc: 0.9907 | test loss: 0.1249, test acc: 0.9908 | 6.63 sec.\n",
      "Epoch: 149, loss: 0.3215, acc: 0.9908 | test loss: 0.1266, test acc: 0.9906 | 6.55 sec.\n",
      "Epoch: 150, loss: 0.3157, acc: 0.9909 | test loss: 0.1279, test acc: 0.9904 | 6.61 sec.\n",
      "Epoch: 151, loss: 0.3167, acc: 0.9909 | test loss: 0.1252, test acc: 0.9908 | 7.05 sec.\n",
      "Epoch: 152, loss: 0.3135, acc: 0.9910 | test loss: 0.1120, test acc: 0.9917 | 6.70 sec.\n",
      "Epoch: 153, loss: 0.3077, acc: 0.9912 | test loss: 0.1138, test acc: 0.9916 | 6.47 sec.\n",
      "Epoch: 154, loss: 0.3067, acc: 0.9912 | test loss: 0.1211, test acc: 0.9910 | 6.70 sec.\n",
      "Epoch: 155, loss: 0.3034, acc: 0.9913 | test loss: 0.1137, test acc: 0.9917 | 6.60 sec.\n",
      "Epoch: 156, loss: 0.3028, acc: 0.9913 | test loss: 0.1198, test acc: 0.9912 | 6.74 sec.\n",
      "Epoch: 157, loss: 0.2989, acc: 0.9914 | test loss: 0.1150, test acc: 0.9914 | 6.60 sec.\n",
      "Epoch: 158, loss: 0.2977, acc: 0.9915 | test loss: 0.1204, test acc: 0.9914 | 6.56 sec.\n",
      "Epoch: 159, loss: 0.2952, acc: 0.9917 | test loss: 0.1118, test acc: 0.9920 | 6.60 sec.\n",
      "Epoch: 160, loss: 0.2952, acc: 0.9918 | test loss: 0.1150, test acc: 0.9917 | 6.51 sec.\n",
      "Epoch: 161, loss: 0.2926, acc: 0.9919 | test loss: 0.1204, test acc: 0.9913 | 6.73 sec.\n",
      "Epoch: 162, loss: 0.2880, acc: 0.9921 | test loss: 0.1121, test acc: 0.9923 | 6.45 sec.\n",
      "Epoch: 163, loss: 0.2880, acc: 0.9922 | test loss: 0.1169, test acc: 0.9920 | 6.52 sec.\n",
      "Epoch: 164, loss: 0.2848, acc: 0.9924 | test loss: 0.1265, test acc: 0.9915 | 6.60 sec.\n",
      "Epoch: 165, loss: 0.2803, acc: 0.9927 | test loss: 0.1177, test acc: 0.9921 | 6.62 sec.\n",
      "Epoch: 166, loss: 0.2780, acc: 0.9929 | test loss: 0.1021, test acc: 0.9933 | 7.72 sec.\n",
      "Epoch: 167, loss: 0.2796, acc: 0.9930 | test loss: 0.1090, test acc: 0.9931 | 6.85 sec.\n",
      "Epoch: 168, loss: 0.2776, acc: 0.9931 | test loss: 0.1099, test acc: 0.9928 | 6.62 sec.\n",
      "Epoch: 169, loss: 0.2722, acc: 0.9934 | test loss: 0.1087, test acc: 0.9934 | 6.67 sec.\n",
      "Epoch: 170, loss: 0.2714, acc: 0.9934 | test loss: 0.1066, test acc: 0.9935 | 6.58 sec.\n",
      "Epoch: 171, loss: 0.2700, acc: 0.9936 | test loss: 0.1037, test acc: 0.9938 | 6.57 sec.\n",
      "Epoch: 172, loss: 0.2673, acc: 0.9938 | test loss: 0.1031, test acc: 0.9939 | 6.57 sec.\n",
      "Epoch: 173, loss: 0.2670, acc: 0.9940 | test loss: 0.1092, test acc: 0.9937 | 6.46 sec.\n",
      "Epoch: 174, loss: 0.2629, acc: 0.9942 | test loss: 0.1054, test acc: 0.9940 | 6.59 sec.\n",
      "Epoch: 175, loss: 0.2611, acc: 0.9943 | test loss: 0.0978, test acc: 0.9943 | 6.67 sec.\n",
      "Epoch: 176, loss: 0.2614, acc: 0.9943 | test loss: 0.1010, test acc: 0.9941 | 6.63 sec.\n",
      "Epoch: 177, loss: 0.2565, acc: 0.9945 | test loss: 0.1116, test acc: 0.9939 | 6.55 sec.\n",
      "Epoch: 178, loss: 0.2557, acc: 0.9946 | test loss: 0.1056, test acc: 0.9942 | 6.54 sec.\n",
      "Epoch: 179, loss: 0.2543, acc: 0.9947 | test loss: 0.1061, test acc: 0.9941 | 6.50 sec.\n",
      "Epoch: 180, loss: 0.2509, acc: 0.9948 | test loss: 0.1061, test acc: 0.9943 | 6.73 sec.\n",
      "Epoch: 181, loss: 0.2512, acc: 0.9948 | test loss: 0.0917, test acc: 0.9950 | 6.77 sec.\n",
      "Epoch: 182, loss: 0.2487, acc: 0.9949 | test loss: 0.0965, test acc: 0.9947 | 6.60 sec.\n",
      "Epoch: 183, loss: 0.2464, acc: 0.9950 | test loss: 0.1021, test acc: 0.9945 | 6.50 sec.\n",
      "Epoch: 184, loss: 0.2464, acc: 0.9950 | test loss: 0.0969, test acc: 0.9949 | 6.66 sec.\n",
      "Epoch: 185, loss: 0.2421, acc: 0.9952 | test loss: 0.0983, test acc: 0.9949 | 6.75 sec.\n",
      "Epoch: 186, loss: 0.2416, acc: 0.9952 | test loss: 0.1012, test acc: 0.9950 | 6.65 sec.\n",
      "Epoch: 187, loss: 0.2405, acc: 0.9953 | test loss: 0.0993, test acc: 0.9950 | 6.72 sec.\n",
      "Epoch: 188, loss: 0.2421, acc: 0.9953 | test loss: 0.1004, test acc: 0.9948 | 6.66 sec.\n",
      "Epoch: 189, loss: 0.2353, acc: 0.9954 | test loss: 0.0861, test acc: 0.9957 | 6.76 sec.\n",
      "Epoch: 190, loss: 0.2348, acc: 0.9955 | test loss: 0.0878, test acc: 0.9957 | 6.78 sec.\n",
      "Epoch: 191, loss: 0.2318, acc: 0.9956 | test loss: 0.0928, test acc: 0.9953 | 7.09 sec.\n",
      "Epoch: 192, loss: 0.2326, acc: 0.9955 | test loss: 0.0881, test acc: 0.9957 | 6.67 sec.\n",
      "Epoch: 193, loss: 0.2295, acc: 0.9956 | test loss: 0.0885, test acc: 0.9957 | 6.48 sec.\n",
      "Epoch: 194, loss: 0.2286, acc: 0.9957 | test loss: 0.0781, test acc: 0.9962 | 6.58 sec.\n",
      "Epoch: 195, loss: 0.2270, acc: 0.9957 | test loss: 0.0910, test acc: 0.9954 | 6.69 sec.\n",
      "Epoch: 196, loss: 0.2262, acc: 0.9957 | test loss: 0.0846, test acc: 0.9957 | 7.01 sec.\n",
      "Epoch: 197, loss: 0.2223, acc: 0.9957 | test loss: 0.0842, test acc: 0.9958 | 6.70 sec.\n",
      "Epoch: 198, loss: 0.2229, acc: 0.9957 | test loss: 0.0807, test acc: 0.9960 | 6.54 sec.\n",
      "Epoch: 199, loss: 0.2217, acc: 0.9958 | test loss: 0.0859, test acc: 0.9956 | 6.56 sec.\n",
      "Epoch: 200, loss: 0.2205, acc: 0.9958 | test loss: 0.0896, test acc: 0.9956 | 6.71 sec.\n",
      "Epoch: 201, loss: 0.2166, acc: 0.9959 | test loss: 0.0887, test acc: 0.9955 | 6.67 sec.\n",
      "Epoch: 202, loss: 0.2164, acc: 0.9959 | test loss: 0.0883, test acc: 0.9957 | 6.56 sec.\n",
      "Epoch: 203, loss: 0.2177, acc: 0.9958 | test loss: 0.0843, test acc: 0.9957 | 6.61 sec.\n",
      "Epoch: 204, loss: 0.2118, acc: 0.9959 | test loss: 0.0834, test acc: 0.9959 | 6.52 sec.\n",
      "Epoch: 205, loss: 0.2122, acc: 0.9959 | test loss: 0.0857, test acc: 0.9957 | 6.43 sec.\n",
      "Epoch: 206, loss: 0.2116, acc: 0.9959 | test loss: 0.0853, test acc: 0.9955 | 6.54 sec.\n",
      "Epoch: 207, loss: 0.2098, acc: 0.9959 | test loss: 0.0889, test acc: 0.9954 | 6.56 sec.\n",
      "Epoch: 208, loss: 0.2080, acc: 0.9960 | test loss: 0.0857, test acc: 0.9958 | 6.64 sec.\n",
      "Epoch: 209, loss: 0.2079, acc: 0.9960 | test loss: 0.0724, test acc: 0.9964 | 6.61 sec.\n",
      "Epoch: 210, loss: 0.2065, acc: 0.9960 | test loss: 0.0783, test acc: 0.9961 | 6.47 sec.\n",
      "Epoch: 211, loss: 0.2045, acc: 0.9960 | test loss: 0.0811, test acc: 0.9959 | 6.55 sec.\n",
      "Epoch: 212, loss: 0.2057, acc: 0.9960 | test loss: 0.0768, test acc: 0.9961 | 6.53 sec.\n",
      "Epoch: 213, loss: 0.2022, acc: 0.9961 | test loss: 0.0810, test acc: 0.9959 | 6.65 sec.\n",
      "Epoch: 214, loss: 0.2010, acc: 0.9961 | test loss: 0.0746, test acc: 0.9963 | 6.72 sec.\n",
      "Epoch: 215, loss: 0.1973, acc: 0.9962 | test loss: 0.0779, test acc: 0.9961 | 6.49 sec.\n",
      "Epoch: 216, loss: 0.1996, acc: 0.9961 | test loss: 0.0760, test acc: 0.9962 | 6.60 sec.\n",
      "Epoch: 217, loss: 0.1998, acc: 0.9961 | test loss: 0.0737, test acc: 0.9961 | 6.86 sec.\n",
      "Epoch: 218, loss: 0.1947, acc: 0.9962 | test loss: 0.0765, test acc: 0.9961 | 6.81 sec.\n",
      "Epoch: 219, loss: 0.1942, acc: 0.9962 | test loss: 0.0791, test acc: 0.9959 | 6.41 sec.\n",
      "Epoch: 220, loss: 0.1943, acc: 0.9962 | test loss: 0.0819, test acc: 0.9956 | 6.52 sec.\n",
      "Epoch: 221, loss: 0.1918, acc: 0.9963 | test loss: 0.0750, test acc: 0.9961 | 6.46 sec.\n",
      "Epoch: 222, loss: 0.1915, acc: 0.9962 | test loss: 0.0792, test acc: 0.9959 | 6.64 sec.\n",
      "Epoch: 223, loss: 0.1892, acc: 0.9963 | test loss: 0.0832, test acc: 0.9957 | 6.65 sec.\n",
      "Epoch: 224, loss: 0.1887, acc: 0.9963 | test loss: 0.0719, test acc: 0.9962 | 6.69 sec.\n",
      "Epoch: 225, loss: 0.1866, acc: 0.9963 | test loss: 0.0749, test acc: 0.9961 | 6.89 sec.\n",
      "Epoch: 226, loss: 0.1872, acc: 0.9963 | test loss: 0.0746, test acc: 0.9963 | 6.73 sec.\n",
      "Epoch: 227, loss: 0.1862, acc: 0.9964 | test loss: 0.0757, test acc: 0.9963 | 6.71 sec.\n",
      "Epoch: 228, loss: 0.1837, acc: 0.9964 | test loss: 0.0711, test acc: 0.9963 | 6.48 sec.\n",
      "Epoch: 229, loss: 0.1828, acc: 0.9965 | test loss: 0.0689, test acc: 0.9965 | 6.45 sec.\n",
      "Epoch: 230, loss: 0.1817, acc: 0.9964 | test loss: 0.0667, test acc: 0.9966 | 6.53 sec.\n",
      "Epoch: 231, loss: 0.1798, acc: 0.9965 | test loss: 0.0714, test acc: 0.9963 | 6.66 sec.\n",
      "Epoch: 232, loss: 0.1798, acc: 0.9965 | test loss: 0.0692, test acc: 0.9964 | 6.61 sec.\n",
      "Epoch: 233, loss: 0.1783, acc: 0.9965 | test loss: 0.0700, test acc: 0.9964 | 6.66 sec.\n",
      "Epoch: 234, loss: 0.1766, acc: 0.9966 | test loss: 0.0724, test acc: 0.9963 | 6.61 sec.\n",
      "Epoch: 235, loss: 0.1794, acc: 0.9965 | test loss: 0.0655, test acc: 0.9966 | 6.52 sec.\n",
      "Epoch: 236, loss: 0.1769, acc: 0.9965 | test loss: 0.0657, test acc: 0.9967 | 6.48 sec.\n",
      "Epoch: 237, loss: 0.1750, acc: 0.9965 | test loss: 0.0641, test acc: 0.9968 | 6.53 sec.\n",
      "Epoch: 238, loss: 0.1742, acc: 0.9966 | test loss: 0.0669, test acc: 0.9965 | 6.75 sec.\n",
      "Epoch: 239, loss: 0.1731, acc: 0.9966 | test loss: 0.0728, test acc: 0.9963 | 6.58 sec.\n",
      "Epoch: 240, loss: 0.1725, acc: 0.9966 | test loss: 0.0747, test acc: 0.9962 | 6.72 sec.\n",
      "Epoch: 241, loss: 0.1708, acc: 0.9967 | test loss: 0.0691, test acc: 0.9965 | 6.55 sec.\n",
      "Epoch: 242, loss: 0.1688, acc: 0.9967 | test loss: 0.0685, test acc: 0.9965 | 6.56 sec.\n",
      "Epoch: 243, loss: 0.1698, acc: 0.9967 | test loss: 0.0641, test acc: 0.9968 | 6.69 sec.\n",
      "Epoch: 244, loss: 0.1679, acc: 0.9967 | test loss: 0.0696, test acc: 0.9965 | 6.75 sec.\n",
      "Epoch: 245, loss: 0.1661, acc: 0.9968 | test loss: 0.0649, test acc: 0.9968 | 6.57 sec.\n",
      "Epoch: 246, loss: 0.1670, acc: 0.9968 | test loss: 0.0692, test acc: 0.9966 | 6.45 sec.\n",
      "Epoch: 247, loss: 0.1655, acc: 0.9968 | test loss: 0.0681, test acc: 0.9965 | 6.53 sec.\n",
      "Epoch: 248, loss: 0.1637, acc: 0.9969 | test loss: 0.0677, test acc: 0.9966 | 6.58 sec.\n",
      "Epoch: 249, loss: 0.1635, acc: 0.9969 | test loss: 0.0679, test acc: 0.9967 | 6.56 sec.\n",
      "Epoch: 250, loss: 0.1611, acc: 0.9969 | test loss: 0.0643, test acc: 0.9967 | 6.55 sec.\n",
      "Epoch: 251, loss: 0.1607, acc: 0.9969 | test loss: 0.0662, test acc: 0.9966 | 6.49 sec.\n",
      "Epoch: 252, loss: 0.1610, acc: 0.9969 | test loss: 0.0666, test acc: 0.9967 | 6.55 sec.\n",
      "Epoch: 253, loss: 0.1608, acc: 0.9970 | test loss: 0.0634, test acc: 0.9967 | 6.76 sec.\n",
      "Epoch: 254, loss: 0.1594, acc: 0.9970 | test loss: 0.0661, test acc: 0.9966 | 6.81 sec.\n",
      "Epoch: 255, loss: 0.1589, acc: 0.9970 | test loss: 0.0611, test acc: 0.9969 | 6.73 sec.\n",
      "Epoch: 256, loss: 0.1574, acc: 0.9970 | test loss: 0.0639, test acc: 0.9970 | 6.55 sec.\n",
      "Epoch: 257, loss: 0.1570, acc: 0.9970 | test loss: 0.0610, test acc: 0.9970 | 6.56 sec.\n",
      "Epoch: 258, loss: 0.1548, acc: 0.9971 | test loss: 0.0596, test acc: 0.9970 | 6.59 sec.\n",
      "Epoch: 259, loss: 0.1538, acc: 0.9971 | test loss: 0.0604, test acc: 0.9970 | 6.71 sec.\n",
      "Epoch: 260, loss: 0.1551, acc: 0.9971 | test loss: 0.0593, test acc: 0.9971 | 6.60 sec.\n",
      "Epoch: 261, loss: 0.1531, acc: 0.9971 | test loss: 0.0590, test acc: 0.9972 | 6.51 sec.\n",
      "Epoch: 262, loss: 0.1533, acc: 0.9971 | test loss: 0.0640, test acc: 0.9969 | 6.45 sec.\n",
      "Epoch: 263, loss: 0.1512, acc: 0.9972 | test loss: 0.0615, test acc: 0.9969 | 6.66 sec.\n",
      "Epoch: 264, loss: 0.1516, acc: 0.9971 | test loss: 0.0565, test acc: 0.9973 | 6.92 sec.\n",
      "Epoch: 265, loss: 0.1496, acc: 0.9972 | test loss: 0.0577, test acc: 0.9972 | 6.90 sec.\n",
      "Epoch: 266, loss: 0.1507, acc: 0.9972 | test loss: 0.0604, test acc: 0.9969 | 6.88 sec.\n",
      "Epoch: 267, loss: 0.1480, acc: 0.9972 | test loss: 0.0569, test acc: 0.9972 | 6.48 sec.\n",
      "Epoch: 268, loss: 0.1487, acc: 0.9972 | test loss: 0.0572, test acc: 0.9971 | 6.79 sec.\n",
      "Epoch: 269, loss: 0.1455, acc: 0.9973 | test loss: 0.0579, test acc: 0.9970 | 6.60 sec.\n",
      "Epoch: 270, loss: 0.1460, acc: 0.9973 | test loss: 0.0584, test acc: 0.9970 | 6.61 sec.\n",
      "Epoch: 271, loss: 0.1447, acc: 0.9973 | test loss: 0.0558, test acc: 0.9971 | 6.64 sec.\n",
      "Epoch: 272, loss: 0.1458, acc: 0.9973 | test loss: 0.0583, test acc: 0.9970 | 6.43 sec.\n",
      "Epoch: 273, loss: 0.1434, acc: 0.9973 | test loss: 0.0548, test acc: 0.9973 | 6.61 sec.\n",
      "Epoch: 274, loss: 0.1428, acc: 0.9973 | test loss: 0.0592, test acc: 0.9972 | 6.77 sec.\n",
      "Epoch: 275, loss: 0.1411, acc: 0.9973 | test loss: 0.0545, test acc: 0.9973 | 6.55 sec.\n",
      "Epoch: 276, loss: 0.1417, acc: 0.9973 | test loss: 0.0581, test acc: 0.9970 | 6.54 sec.\n",
      "Epoch: 277, loss: 0.1406, acc: 0.9973 | test loss: 0.0561, test acc: 0.9971 | 6.57 sec.\n",
      "Epoch: 278, loss: 0.1389, acc: 0.9974 | test loss: 0.0568, test acc: 0.9971 | 6.57 sec.\n",
      "Epoch: 279, loss: 0.1401, acc: 0.9974 | test loss: 0.0559, test acc: 0.9973 | 6.61 sec.\n",
      "Epoch: 280, loss: 0.1397, acc: 0.9974 | test loss: 0.0654, test acc: 0.9966 | 6.78 sec.\n",
      "Epoch: 281, loss: 0.1374, acc: 0.9974 | test loss: 0.0524, test acc: 0.9974 | 6.48 sec.\n",
      "Epoch: 282, loss: 0.1376, acc: 0.9974 | test loss: 0.0509, test acc: 0.9975 | 6.50 sec.\n",
      "Epoch: 283, loss: 0.1365, acc: 0.9974 | test loss: 0.0508, test acc: 0.9975 | 6.76 sec.\n",
      "Epoch: 284, loss: 0.1361, acc: 0.9974 | test loss: 0.0550, test acc: 0.9972 | 6.64 sec.\n",
      "Epoch: 285, loss: 0.1362, acc: 0.9974 | test loss: 0.0517, test acc: 0.9974 | 6.77 sec.\n",
      "Epoch: 286, loss: 0.1352, acc: 0.9974 | test loss: 0.0531, test acc: 0.9973 | 6.63 sec.\n",
      "Epoch: 287, loss: 0.1345, acc: 0.9974 | test loss: 0.0480, test acc: 0.9977 | 6.49 sec.\n",
      "Epoch: 288, loss: 0.1344, acc: 0.9974 | test loss: 0.0481, test acc: 0.9976 | 6.63 sec.\n",
      "Epoch: 289, loss: 0.1321, acc: 0.9975 | test loss: 0.0519, test acc: 0.9973 | 6.58 sec.\n",
      "Epoch: 290, loss: 0.1330, acc: 0.9974 | test loss: 0.0561, test acc: 0.9970 | 6.59 sec.\n",
      "Epoch: 291, loss: 0.1321, acc: 0.9975 | test loss: 0.0566, test acc: 0.9971 | 6.66 sec.\n",
      "Epoch: 292, loss: 0.1318, acc: 0.9975 | test loss: 0.0494, test acc: 0.9974 | 6.46 sec.\n",
      "Epoch: 293, loss: 0.1308, acc: 0.9975 | test loss: 0.0496, test acc: 0.9975 | 6.52 sec.\n",
      "Epoch: 294, loss: 0.1297, acc: 0.9975 | test loss: 0.0529, test acc: 0.9975 | 6.52 sec.\n",
      "Epoch: 295, loss: 0.1286, acc: 0.9975 | test loss: 0.0483, test acc: 0.9976 | 6.77 sec.\n",
      "Epoch: 296, loss: 0.1276, acc: 0.9976 | test loss: 0.0514, test acc: 0.9974 | 6.73 sec.\n",
      "Epoch: 297, loss: 0.1267, acc: 0.9976 | test loss: 0.0486, test acc: 0.9976 | 6.55 sec.\n",
      "Epoch: 298, loss: 0.1280, acc: 0.9976 | test loss: 0.0541, test acc: 0.9972 | 6.56 sec.\n",
      "Epoch: 299, loss: 0.1270, acc: 0.9976 | test loss: 0.0520, test acc: 0.9974 | 6.74 sec.\n",
      "Epoch: 300, loss: 0.1263, acc: 0.9976 | test loss: 0.0512, test acc: 0.9972 | 6.74 sec.\n",
      "Epoch: 301, loss: 0.1249, acc: 0.9976 | test loss: 0.0540, test acc: 0.9972 | 6.65 sec.\n",
      "Epoch: 302, loss: 0.1249, acc: 0.9976 | test loss: 0.0464, test acc: 0.9978 | 6.61 sec.\n",
      "Epoch: 303, loss: 0.1241, acc: 0.9977 | test loss: 0.0477, test acc: 0.9976 | 7.24 sec.\n",
      "Epoch: 304, loss: 0.1236, acc: 0.9977 | test loss: 0.0479, test acc: 0.9977 | 6.68 sec.\n",
      "Epoch: 305, loss: 0.1227, acc: 0.9977 | test loss: 0.0534, test acc: 0.9973 | 6.56 sec.\n",
      "Epoch: 306, loss: 0.1229, acc: 0.9977 | test loss: 0.0468, test acc: 0.9976 | 6.69 sec.\n",
      "Epoch: 307, loss: 0.1228, acc: 0.9977 | test loss: 0.0488, test acc: 0.9975 | 6.45 sec.\n",
      "Epoch: 308, loss: 0.1231, acc: 0.9977 | test loss: 0.0462, test acc: 0.9977 | 6.44 sec.\n",
      "Epoch: 309, loss: 0.1194, acc: 0.9978 | test loss: 0.0504, test acc: 0.9974 | 6.54 sec.\n",
      "Epoch: 310, loss: 0.1194, acc: 0.9977 | test loss: 0.0486, test acc: 0.9976 | 6.58 sec.\n",
      "Epoch: 311, loss: 0.1205, acc: 0.9977 | test loss: 0.0485, test acc: 0.9975 | 7.40 sec.\n",
      "Epoch: 312, loss: 0.1204, acc: 0.9977 | test loss: 0.0496, test acc: 0.9975 | 7.16 sec.\n",
      "Epoch: 313, loss: 0.1199, acc: 0.9977 | test loss: 0.0483, test acc: 0.9976 | 6.57 sec.\n",
      "Epoch: 314, loss: 0.1189, acc: 0.9978 | test loss: 0.0468, test acc: 0.9978 | 6.64 sec.\n",
      "Epoch: 315, loss: 0.1179, acc: 0.9978 | test loss: 0.0448, test acc: 0.9978 | 6.56 sec.\n",
      "Epoch: 316, loss: 0.1176, acc: 0.9978 | test loss: 0.0461, test acc: 0.9977 | 7.12 sec.\n",
      "Epoch: 317, loss: 0.1157, acc: 0.9978 | test loss: 0.0474, test acc: 0.9977 | 6.46 sec.\n",
      "Epoch: 318, loss: 0.1160, acc: 0.9978 | test loss: 0.0467, test acc: 0.9977 | 6.60 sec.\n",
      "Epoch: 319, loss: 0.1164, acc: 0.9978 | test loss: 0.0437, test acc: 0.9978 | 6.45 sec.\n",
      "Epoch: 320, loss: 0.1148, acc: 0.9978 | test loss: 0.0451, test acc: 0.9977 | 6.68 sec.\n",
      "Epoch: 321, loss: 0.1152, acc: 0.9978 | test loss: 0.0495, test acc: 0.9975 | 6.50 sec.\n",
      "Epoch: 322, loss: 0.1140, acc: 0.9978 | test loss: 0.0450, test acc: 0.9978 | 6.89 sec.\n",
      "Epoch: 323, loss: 0.1126, acc: 0.9978 | test loss: 0.0416, test acc: 0.9979 | 6.61 sec.\n",
      "Epoch: 324, loss: 0.1138, acc: 0.9978 | test loss: 0.0451, test acc: 0.9976 | 6.42 sec.\n",
      "Epoch: 325, loss: 0.1137, acc: 0.9978 | test loss: 0.0438, test acc: 0.9978 | 6.69 sec.\n",
      "Epoch: 326, loss: 0.1128, acc: 0.9978 | test loss: 0.0434, test acc: 0.9978 | 6.47 sec.\n",
      "Epoch: 327, loss: 0.1124, acc: 0.9978 | test loss: 0.0439, test acc: 0.9979 | 6.59 sec.\n",
      "Epoch: 328, loss: 0.1126, acc: 0.9978 | test loss: 0.0448, test acc: 0.9976 | 6.43 sec.\n",
      "Epoch: 329, loss: 0.1108, acc: 0.9979 | test loss: 0.0431, test acc: 0.9976 | 6.55 sec.\n",
      "Epoch: 330, loss: 0.1097, acc: 0.9979 | test loss: 0.0456, test acc: 0.9977 | 6.55 sec.\n",
      "Epoch: 331, loss: 0.1104, acc: 0.9978 | test loss: 0.0453, test acc: 0.9976 | 6.56 sec.\n",
      "Epoch: 332, loss: 0.1105, acc: 0.9978 | test loss: 0.0432, test acc: 0.9979 | 6.71 sec.\n",
      "Epoch: 333, loss: 0.1098, acc: 0.9979 | test loss: 0.0451, test acc: 0.9976 | 6.50 sec.\n",
      "Epoch: 334, loss: 0.1088, acc: 0.9979 | test loss: 0.0452, test acc: 0.9978 | 6.48 sec.\n",
      "Epoch: 335, loss: 0.1092, acc: 0.9978 | test loss: 0.0450, test acc: 0.9976 | 6.70 sec.\n",
      "Epoch: 336, loss: 0.1086, acc: 0.9978 | test loss: 0.0428, test acc: 0.9978 | 6.72 sec.\n",
      "Epoch: 337, loss: 0.1066, acc: 0.9979 | test loss: 0.0429, test acc: 0.9977 | 6.80 sec.\n",
      "Epoch: 338, loss: 0.1075, acc: 0.9979 | test loss: 0.0416, test acc: 0.9979 | 6.53 sec.\n",
      "Epoch: 339, loss: 0.1071, acc: 0.9979 | test loss: 0.0423, test acc: 0.9979 | 6.38 sec.\n",
      "Epoch: 340, loss: 0.1062, acc: 0.9979 | test loss: 0.0424, test acc: 0.9978 | 6.54 sec.\n",
      "Epoch: 341, loss: 0.1058, acc: 0.9979 | test loss: 0.0429, test acc: 0.9978 | 6.65 sec.\n",
      "Epoch: 342, loss: 0.1057, acc: 0.9979 | test loss: 0.0447, test acc: 0.9974 | 6.62 sec.\n",
      "Epoch: 343, loss: 0.1061, acc: 0.9979 | test loss: 0.0429, test acc: 0.9977 | 6.54 sec.\n",
      "Epoch: 344, loss: 0.1050, acc: 0.9979 | test loss: 0.0403, test acc: 0.9979 | 6.48 sec.\n",
      "Epoch: 345, loss: 0.1040, acc: 0.9980 | test loss: 0.0417, test acc: 0.9979 | 6.51 sec.\n",
      "Epoch: 346, loss: 0.1046, acc: 0.9979 | test loss: 0.0420, test acc: 0.9978 | 6.67 sec.\n",
      "Epoch: 347, loss: 0.1031, acc: 0.9980 | test loss: 0.0435, test acc: 0.9976 | 6.78 sec.\n",
      "Epoch: 348, loss: 0.1020, acc: 0.9980 | test loss: 0.0415, test acc: 0.9978 | 6.85 sec.\n",
      "Epoch: 349, loss: 0.1021, acc: 0.9980 | test loss: 0.0412, test acc: 0.9979 | 6.49 sec.\n",
      "Epoch: 350, loss: 0.1033, acc: 0.9980 | test loss: 0.0394, test acc: 0.9981 | 6.73 sec.\n",
      "Epoch: 351, loss: 0.1005, acc: 0.9980 | test loss: 0.0410, test acc: 0.9979 | 6.57 sec.\n",
      "Epoch: 352, loss: 0.1019, acc: 0.9980 | test loss: 0.0385, test acc: 0.9980 | 6.54 sec.\n",
      "Epoch: 353, loss: 0.1020, acc: 0.9980 | test loss: 0.0363, test acc: 0.9982 | 6.81 sec.\n",
      "Epoch: 354, loss: 0.1015, acc: 0.9980 | test loss: 0.0438, test acc: 0.9978 | 6.44 sec.\n",
      "Epoch: 355, loss: 0.0998, acc: 0.9981 | test loss: 0.0389, test acc: 0.9980 | 6.45 sec.\n",
      "Epoch: 356, loss: 0.1000, acc: 0.9980 | test loss: 0.0426, test acc: 0.9977 | 6.49 sec.\n",
      "Epoch: 357, loss: 0.0996, acc: 0.9981 | test loss: 0.0424, test acc: 0.9977 | 6.54 sec.\n",
      "Epoch: 358, loss: 0.0997, acc: 0.9981 | test loss: 0.0409, test acc: 0.9979 | 6.69 sec.\n",
      "Epoch: 359, loss: 0.0989, acc: 0.9980 | test loss: 0.0388, test acc: 0.9979 | 6.59 sec.\n",
      "Epoch: 360, loss: 0.0988, acc: 0.9980 | test loss: 0.0373, test acc: 0.9980 | 6.57 sec.\n",
      "Epoch: 361, loss: 0.0990, acc: 0.9980 | test loss: 0.0420, test acc: 0.9979 | 6.53 sec.\n",
      "Epoch: 362, loss: 0.0980, acc: 0.9981 | test loss: 0.0377, test acc: 0.9981 | 6.67 sec.\n",
      "Epoch: 363, loss: 0.0975, acc: 0.9981 | test loss: 0.0382, test acc: 0.9980 | 6.73 sec.\n",
      "Epoch: 364, loss: 0.0974, acc: 0.9981 | test loss: 0.0390, test acc: 0.9981 | 6.60 sec.\n",
      "Epoch: 365, loss: 0.0966, acc: 0.9981 | test loss: 0.0378, test acc: 0.9980 | 6.48 sec.\n",
      "Epoch: 366, loss: 0.0969, acc: 0.9981 | test loss: 0.0393, test acc: 0.9980 | 6.49 sec.\n",
      "Epoch: 367, loss: 0.0966, acc: 0.9981 | test loss: 0.0399, test acc: 0.9979 | 6.58 sec.\n",
      "Epoch: 368, loss: 0.0958, acc: 0.9981 | test loss: 0.0382, test acc: 0.9979 | 6.66 sec.\n",
      "Epoch: 369, loss: 0.0967, acc: 0.9981 | test loss: 0.0382, test acc: 0.9980 | 6.71 sec.\n",
      "Epoch: 370, loss: 0.0951, acc: 0.9981 | test loss: 0.0369, test acc: 0.9981 | 6.40 sec.\n",
      "Epoch: 371, loss: 0.0942, acc: 0.9981 | test loss: 0.0353, test acc: 0.9982 | 6.48 sec.\n",
      "Epoch: 372, loss: 0.0949, acc: 0.9981 | test loss: 0.0394, test acc: 0.9979 | 6.56 sec.\n",
      "Epoch: 373, loss: 0.0938, acc: 0.9981 | test loss: 0.0350, test acc: 0.9981 | 6.57 sec.\n",
      "Epoch: 374, loss: 0.0939, acc: 0.9981 | test loss: 0.0353, test acc: 0.9980 | 6.65 sec.\n",
      "Epoch: 375, loss: 0.0940, acc: 0.9981 | test loss: 0.0362, test acc: 0.9980 | 6.33 sec.\n",
      "Epoch: 376, loss: 0.0934, acc: 0.9981 | test loss: 0.0368, test acc: 0.9980 | 6.44 sec.\n",
      "Epoch: 377, loss: 0.0937, acc: 0.9981 | test loss: 0.0361, test acc: 0.9981 | 6.81 sec.\n",
      "Epoch: 378, loss: 0.0929, acc: 0.9981 | test loss: 0.0376, test acc: 0.9980 | 6.89 sec.\n",
      "Epoch: 379, loss: 0.0914, acc: 0.9982 | test loss: 0.0349, test acc: 0.9982 | 6.48 sec.\n",
      "Epoch: 380, loss: 0.0927, acc: 0.9981 | test loss: 0.0355, test acc: 0.9982 | 6.56 sec.\n",
      "Epoch: 381, loss: 0.0920, acc: 0.9981 | test loss: 0.0355, test acc: 0.9981 | 6.35 sec.\n",
      "Epoch: 382, loss: 0.0911, acc: 0.9981 | test loss: 0.0399, test acc: 0.9978 | 6.45 sec.\n",
      "Epoch: 383, loss: 0.0910, acc: 0.9981 | test loss: 0.0363, test acc: 0.9980 | 6.54 sec.\n",
      "Epoch: 384, loss: 0.0902, acc: 0.9982 | test loss: 0.0365, test acc: 0.9981 | 6.51 sec.\n",
      "Epoch: 385, loss: 0.0902, acc: 0.9982 | test loss: 0.0349, test acc: 0.9981 | 6.39 sec.\n",
      "Epoch: 386, loss: 0.0913, acc: 0.9981 | test loss: 0.0356, test acc: 0.9981 | 6.44 sec.\n",
      "Epoch: 387, loss: 0.0904, acc: 0.9981 | test loss: 0.0355, test acc: 0.9981 | 6.54 sec.\n",
      "Epoch: 388, loss: 0.0891, acc: 0.9982 | test loss: 0.0334, test acc: 0.9982 | 6.47 sec.\n",
      "Epoch: 389, loss: 0.0897, acc: 0.9981 | test loss: 0.0349, test acc: 0.9981 | 6.57 sec.\n",
      "Epoch: 390, loss: 0.0883, acc: 0.9982 | test loss: 0.0352, test acc: 0.9982 | 6.42 sec.\n",
      "Epoch: 391, loss: 0.0884, acc: 0.9982 | test loss: 0.0370, test acc: 0.9980 | 6.35 sec.\n",
      "Epoch: 392, loss: 0.0892, acc: 0.9982 | test loss: 0.0338, test acc: 0.9982 | 6.52 sec.\n",
      "Epoch: 393, loss: 0.0877, acc: 0.9982 | test loss: 0.0354, test acc: 0.9981 | 6.57 sec.\n",
      "Epoch: 394, loss: 0.0875, acc: 0.9982 | test loss: 0.0343, test acc: 0.9981 | 6.63 sec.\n",
      "Epoch: 395, loss: 0.0872, acc: 0.9982 | test loss: 0.0365, test acc: 0.9980 | 6.39 sec.\n",
      "Epoch: 396, loss: 0.0873, acc: 0.9982 | test loss: 0.0356, test acc: 0.9981 | 6.93 sec.\n",
      "Epoch: 397, loss: 0.0871, acc: 0.9982 | test loss: 0.0369, test acc: 0.9980 | 6.41 sec.\n",
      "Epoch: 398, loss: 0.0865, acc: 0.9982 | test loss: 0.0338, test acc: 0.9982 | 6.47 sec.\n",
      "Epoch: 399, loss: 0.0862, acc: 0.9982 | test loss: 0.0340, test acc: 0.9980 | 6.49 sec.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc, iter_num = .0, .0, .0\n",
    "    start_epoch_time = time.time()\n",
    "    model.train()\n",
    "    for x_in, y_in in train_dl:\n",
    "        x_in = x_in\n",
    "        y_in = y_in.view(1, -1).squeeze()\n",
    "        optimizer.zero_grad()\n",
    "        out = model.forward(x_in).view(-1, len(CHARS))\n",
    "        l = criterion(out, y_in)\n",
    "        train_loss += l.item()\n",
    "        batch_acc = (out.argmax(dim=1) == y_in)\n",
    "        train_acc += batch_acc.sum().item() / batch_acc.shape[0]\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        iter_num += 1\n",
    "    print(\n",
    "        f\"Epoch: {epoch}, loss: {train_loss:.4f}, acc: \"\n",
    "        f\"{train_acc / iter_num:.4f}\",\n",
    "        end=\" | \"\n",
    "    )\n",
    "    test_loss, test_acc, iter_num = .0, .0, .0\n",
    "    model.eval()\n",
    "    for x_in, y_in in test_dl:\n",
    "        x_in = x_in\n",
    "        y_in = y_in.view(1, -1).squeeze()\n",
    "        out = model.forward(x_in).view(-1, len(CHARS))\n",
    "        l = criterion(out, y_in)\n",
    "        test_loss += l.item()\n",
    "        batch_acc = (out.argmax(dim=1) == y_in)\n",
    "        test_acc += batch_acc.sum().item() / batch_acc.shape[0]\n",
    "        iter_num += 1\n",
    "    print(\n",
    "        f\"test loss: {test_loss:.4f}, test acc: {test_acc / iter_num:.4f} | \"\n",
    "        f\"{time.time() - start_epoch_time:.2f} sec.\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  in a recurrent neural network we store the output activations from one or more of the layers of the network\n",
      "Encrypted:  kpbcbtgewttgpvbpgwtcnbpgvyqtmbygbuvqtgbvjgbqwvrwvbcevkxcvkqpubhtqobqpgbqtboqtgbqhbvjgbnc gtubqhbvjgbpgvyqtm\n",
      "Deencrypted:  in a recurrent neural networs we store the output activations from one or more of the layers of the networs\n",
      "Accuracy=0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-9d6789842bd6>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_acc = (result == torch.tensor(phrase_idx)).flatten()\n"
     ]
    }
   ],
   "source": [
    "def encoder(phrase):\n",
    "    return ''.join([caesor(k, CAESAR_OFFSET) for k in phrase])\n",
    "\n",
    "def evaluate(phrase, model):\n",
    "    print('Original: ', phrase)\n",
    "\n",
    "    phrase_idx = torch.zeros(len(phrase), dtype=int)\n",
    "    for i in range(len(phrase)):\n",
    "        phrase_idx[i] = CHAR_TO_INDEX.get(phrase[i], CHAR_TO_INDEX[' '])\n",
    "    enc_phrase = encoder(phrase)\n",
    "    print('Encrypted: ', enc_phrase)\n",
    "\n",
    "    enc_phrase_idx = [CHAR_TO_INDEX[k] for k in enc_phrase]\n",
    "    model.eval()\n",
    "    result = model.forward(torch.tensor([enc_phrase_idx])).argmax(dim=2)\n",
    "    encoded_result = \"\".join([INDEX_TO_CHAR[item.item()] for i, item in enumerate(result[0])])\n",
    "    print('Deencrypted: ', encoded_result)\n",
    "\n",
    "    val_acc = (result == torch.tensor(phrase_idx)).flatten()\n",
    "    val_acc = (val_acc.sum() / val_acc.shape[0]).item()\n",
    "    print('Accuracy={:.2f}'.format(val_acc))\n",
    "\n",
    "phrase = 'in a recurrent neural network we store the output activations from one or more of the layers of the network'\n",
    "evaluate(phrase, model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "23ceb7112fbf9d0e38ecbf60d6e6d5e2dcebcc82200eeb1e5a5d5f9ffb9e27ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}